{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Введение в машинное обучение\n",
    "\n",
    "## Семинар #9\n",
    "\n",
    "### Екатерина Кондратьева\n",
    "\n",
    "ekaterina.kondrateva@skoltech.ru\n",
    "\n",
    "## Обучение без учителя: кластеризация. Снижение размерности данных PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Обучение без учителя: Kластеризация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластерный анализ (англ. cluster analysis) — многомерная статистическая процедура, выполняющая сбор данных, содержащих информацию о выборке объектов, и затем упорядочивающая объекты в сравнительно однородные группы. Задача кластеризации относится к статистической обработке, а также к широкому классу задач обучения без учителя.\n",
    "\n",
    "Источники:\n",
    "1. Лекция https://ru.coursera.org/lecture/unsupervised-learning/primier-klastierizatsiia-tiekstov-po-tiemie-bVVzw \n",
    "2. https://ru.coursera.org/lecture/python-for-data-science/mietod-glavnykh-komponient-principal-component-analysis-X8bem\n",
    "2. https://ru.coursera.org/lecture/unsupervised-learning/otbor-priznakov-na-osnovie-modieliei-qnzXA\n",
    "3. https://www.hse.ru/mirror/pubs/share/215285956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear algebra\n",
    "import numpy as np\n",
    "#data structures\n",
    "import pandas as pd\n",
    "#ml models\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVR\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#beautiful plots\n",
    "import seaborn as sns\n",
    "#linear regression\n",
    "import statsmodels.api as sm\n",
    "#off the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier     #KNN\n",
    "from sklearn.linear_model import LogisticRegression    #Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. В предыдущих занятиях:\n",
    "\n",
    " Мультикласс классификация на выборке ирисов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=[0, 1]\n",
    "X = X_train[:, [0, 1]]\n",
    "y = y_train\n",
    "\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.005\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=10,  \n",
    "                           p=2).fit(X, y)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "print(clf.score(X_test[:, [0, 1]], y_test))\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что, если мы не знаем, сколько классов в нашей выборке?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм KMeans:\n",
    "https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%85\n",
    "\n",
    "\n",
    "Реализация:\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux, Modified for documentation by Jaques Grobler, License: BSD 3 clause\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.style\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(42)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_3', KMeans(n_clusters=3)),]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(8, 8))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=120)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(8, 8))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=120)\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда мы знали, что нужно искать 3-8 кластеров `kmeans`? \n",
    "\n",
    "\n",
    "СМЕКАЛОЧКА! Мы угадали сколько  выбрать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, v_measure_score\n",
    "silhouette_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(X)\n",
    "labels = est.labels_\n",
    "silhouette_score(pd.DataFrame(labels),pd.DataFrame(y.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_measure_score(labels,y.astype(int)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "AgglomerativeClustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "estimators = [('AgglomerativeClustering_iris_8', AgglomerativeClustering(n_clusters=8)),\n",
    "              ('AgglomerativeClustering_iris_3', AgglomerativeClustering(n_clusters=3)),\n",
    "              ('AgglomerativeClustering_iris_2', AgglomerativeClustering(n_clusters=2 ))]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters', '2 clusters']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(8, 8))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=120)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(8, 8))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=120)\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "    \n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y.astype(int), [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будет меняться оценка от размера класса?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А что, если мы не хотим заниматься перебором. Как оценить втурненнюю размерность выборки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья на NIPS 2004: https://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
    " \n",
    "how to use\n",
    "----------\n",
    " \n",
    "The goal is to estimate intrinsic dimensionality of data, the estimation of dimensionality is scale dependent\n",
    "(depending on how much you zoom into the data distribution you can find different dimesionality), so they\n",
    "propose to average it over different scales, the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
    " \n",
    "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
    " \n",
    "Here is one example with swiss roll :\n",
    " \n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, _ = make_swiss_roll(1000)\n",
    " \n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=500, # nb_iter for bootstrapping\n",
    "                             verbose=1, \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where \n",
    "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
    "# is (k2 - k1 + 1).\n",
    " \n",
    "# Plotting the histogram of intrinsic dimensionality estimations repeated over\n",
    "# nb_iter experiments\n",
    "plt.hist(intdim_k_repeated.mean(axis=1))\n",
    " \n",
    "\"\"\"\n",
    "# from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    dist, ind = neighb.kneighbors(X) # distances between the samples and points\n",
    "    dist = dist[:, 1:] # the distance between the first points to first points (as basis ) equals zero\n",
    "    # the first non trivial point\n",
    "    dist = dist[:, 0:k]# including points k-1\n",
    "    assert dist.shape == (X.shape[0], k) # requirments are there is no equal points\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1]) # dinstance betveen the bayeasan statistics\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1): # in order to reduse the noise by eliminating of the nearest neibours \n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':# and each point we want to resample with repeating points to reduse the errors \n",
    "            #232 111 133 \n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 10 # start of interval(included)\n",
    "k2 = 30 # end of interval(included)\n",
    "nb_iter = 10# more iterations more accuracy\n",
    "# intrinsic_dim_scale_interval gives better estimation\n",
    "X = iris.data\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "\n",
    "print (np.shape(intdim_k_repeated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0),'b') # it is the mean walue\n",
    "# Для наглядности построим  СТД\n",
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0)+ np.std(intdim_k_repeated, axis=0),'r')\n",
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0)- np.std(intdim_k_repeated, axis=0),'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Снижение размерности данных PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лекция: \n",
    "    https://ru.coursera.org/lecture/unsupervised-learning/mietod-ghlavnykh-komponient-rieshieniie-e72bH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. В предыдущих занятиях:\n",
    "\n",
    "Детектирование аномалий в выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/creditcard.csv\")\n",
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class PCAAnomalyDetector(BaseEstimator):\n",
    "    def __init__(self, n_components=2, contamination=0.1):\n",
    "        self.n_components = n_components\n",
    "        self.contamination = contamination\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.pca_model_ = PCA(n_components=self.n_components)\n",
    "        self.pca_model_.fit(data)\n",
    "\n",
    "        low_dim_data = self.pca_model_.transform(data)\n",
    "        new_data = self.pca_model_.inverse_transform(low_dim_data)\n",
    "\n",
    "        distances = np.linalg.norm(data - new_data, axis=1)\n",
    "\n",
    "        self.threshold_ = -np.percentile(distances, 100 - self.contamination * 100)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, data):\n",
    "        low_dim_data = self.pca_model_.transform(data)\n",
    "        new_data = self.pca_model_.inverse_transform(low_dim_data)\n",
    "\n",
    "        return - np.linalg.norm(data - new_data, axis=1)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        scores = self.decision_function(data)\n",
    "        return np.where(scores > self.threshold_, +1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def data_generator(n_samples=100, anomaly_fraction=0.1, n_features=2):\n",
    "    n_anomaly = int(n_samples * anomaly_fraction)\n",
    "    n_normal = n_samples - n_anomaly\n",
    "\n",
    "    normal_data, _ = make_blobs(n_normal, n_features=n_features, centers=3)\n",
    "\n",
    "    anomaly_data = np.random.rand(n_anomaly, n_features)\n",
    "\n",
    "    nrm_min = normal_data.min(axis=0).reshape(1, -1)\n",
    "    nrm_ptp = normal_data.ptp(axis=0).reshape(1, -1)\n",
    "    anomaly_data = anomaly_data * nrm_ptp + nrm_min\n",
    "\n",
    "    return np.concatenate([normal_data, anomaly_data], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# author @artonson\n",
    "\n",
    "def plot_clustering(model, data, size=100):\n",
    "    def _expand(a, b, frac=.5, margin=1.):\n",
    "        return a - abs(a) * frac - margin, b + abs(b) * frac + margin\n",
    "\n",
    "    # Вспомогательная функция для рисования линий уровня и набора точек\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    min_x, min_y = data.min(axis=0)\n",
    "    max_x, max_y = data.max(axis=0)\n",
    "    min_x, max_x = _expand(min_x, max_x)\n",
    "    min_y, max_y = _expand(min_y, max_y)\n",
    "\n",
    "    # создаём регулярную сетку для контуров\n",
    "    all_x = np.linspace(min_x, max_x, num=size)\n",
    "    all_y = np.linspace(min_y, max_y, num=size)\n",
    "    XX, YY = np.meshgrid(all_x, all_y)\n",
    "    test_data = np.c_[XX.ravel(), YY.ravel()]\n",
    "\n",
    "    # опрашиваем предсказания модели\n",
    "    try:\n",
    "        predictions = model.decision_function(test_data).reshape(size, size)\n",
    "        data_scores = model.predict(data)\n",
    "        anomaly_scores = model.decision_function(data)\n",
    "\n",
    "    except AttributeError:\n",
    "        predictions = model._decision_function(test_data).reshape(size, size)\n",
    "        data_scores = model._predict(data)\n",
    "        anomaly_scores = model._decision_function(data)\n",
    "\n",
    "    # создаём график контуров с заливкоц\n",
    "    plt.contourf(all_x, all_y, predictions, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    # отображаем границу принятия решений\n",
    "    threshold = anomaly_scores[data_scores==1.0].min()\n",
    "    plt.contour(XX, YY, predictions, levels=[threshold], linewidths=1)\n",
    "\n",
    "    # нарисуем точки выборки\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([min_x,max_x])\n",
    "    axes.set_ylim([min_y,max_y])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blobs = data_generator()\n",
    "model = PCAAnomalyDetector(n_components=1)\n",
    "model.fit(data_blobs)\n",
    "\n",
    "plot_clustering(model, data_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCAAnomalyDetector(n_components=12, contamination=0.01)\n",
    "\n",
    "model.fit(train_X)\n",
    "predictions_pca = -model.decision_function(test_X)\n",
    "\n",
    "labels_pca = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(test_y, (labels_pca < 0) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим, как это работает на ирисах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 7))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=120)\n",
    "\n",
    "plt.cla()\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,\n",
    "           edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA на картинках, датасет `Handwritten digits`:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.unique(digits.target))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data[8].reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(digits.data[8].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем PCA ручками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size, sample_dim = np.shape(digits.data)\n",
    "sample_mean = np.mean(digits.data, axis = 0)       # Data mean as list.\n",
    "centered_data = digits.data - sample_mean\n",
    "\n",
    "sample_covariance = np.dot(centered_data.transpose(), centered_data) / sample_size  # Data covariance estimation as a 2x2 dimensional array.\n",
    "\n",
    "lambdas, eigen_vectors = np.linalg.eigh(sample_covariance)     # Principal components as column (!) vectors in a 2x2 dimensional array.\n",
    "\n",
    "print(sample_size, sample_dim)\n",
    "print(\"\\nNumerical mean of centered data (should be zero):\\n\", np.mean(centered_data, axis = 0)) # Verify that data has zero mean.\n",
    "\n",
    "lambdas = lambdas[-1::-1]\n",
    "eigen_vectors = eigen_vectors[:, -1::-1]\n",
    "print(\"\\nVariances:\\n\", lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация PCA на датасете Handwritten Digits: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"сколько\" дисперсии объясняет каждая компонента\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(sample_dim), np.cumsum(lambdas) / np.sum(lambdas), \"-*b\", label = 'cumsum')\n",
    "plt.plot(range(sample_dim), [1]*sample_dim, \"r\")\n",
    "plt.plot(range(sample_dim), [0.99]*sample_dim, \"m\")\n",
    "plt.plot(range(sample_dim), [0.95]*sample_dim, \"g\")\n",
    "plt.legend(['cumsum', '1', '0.99', '0.95'])\n",
    "print(np.cumsum(lambdas) / np.sum(lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какая внутреннняя размерность выборки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 10 # start of interval(included)\n",
    "k2 = 15 # end of interval(included)\n",
    "nb_iter = 10 # more iterations more accuracy\n",
    "# intrinsic_dim_scale_interval gives better estimation\n",
    "X = digits.data\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "\n",
    "print (np.shape(intdim_k_repeated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0),'b') # it is the mean walue\n",
    "# Для наглядности построим  СТД\n",
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0)+ np.std(intdim_k_repeated, axis=0),'r')\n",
    "plt.plot(range(k1,k2+1), np.mean(intdim_k_repeated, axis=0)- np.std(intdim_k_repeated, axis=0),'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  А какую размерность мы ожидали здесь увидеть?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластерный анализ Датасет `Handwritten Digits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "labels = digits.target\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(82 * '_')\n",
    "\n",
    "# #############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Геометрические методы снижения размерности Manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация нелинейных методов снижения размерности на датасете `Handwritten digits`:\n",
    "https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n",
    "#          Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#          Mathieu Blondel <mathieu@mblondel.org>\n",
    "#          Gael Varoquaux\n",
    "# License: BSD 3 clause (C) INRIA 2011\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "\n",
    "plt.figure(figsize=[8,10])\n",
    "#----------------------------------------------------------------------\n",
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot images of the digits\n",
    "n_img_per_row = 20\n",
    "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
    "\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('A selection from the 64-dimensional digits dataset')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Random 2D projection using a random unitary matrix\n",
    "print(\"Computing random projection\")\n",
    "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
    "X_projected = rp.fit_transform(X)\n",
    "plot_embedding(X_projected, \"Random Projection of the digits\")\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Projection on to the first 2 principal components\n",
    "\n",
    "print(\"Computing PCA projection\")\n",
    "t0 = time()\n",
    "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\n",
    "plot_embedding(X_pca,\n",
    "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Projection on to the first 2 linear discriminant components\n",
    "\n",
    "print(\"Computing Linear Discriminant Analysis projection\")\n",
    "X2 = X.copy()\n",
    "X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\n",
    "t0 = time()\n",
    "X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)\n",
    "plot_embedding(X_lda,\n",
    "               \"Linear Discriminant projection of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Isomap projection of the digits dataset\n",
    "print(\"Computing Isomap embedding\")\n",
    "t0 = time()\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "plot_embedding(X_iso,\n",
    "               \"Isomap projection of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Locally linear embedding of the digits dataset\n",
    "print(\"Computing LLE embedding\")\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='standard')\n",
    "t0 = time()\n",
    "X_lle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plot_embedding(X_lle,\n",
    "               \"Locally Linear Embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Modified Locally linear embedding of the digits dataset\n",
    "print(\"Computing modified LLE embedding\")\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='modified')\n",
    "t0 = time()\n",
    "X_mlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plot_embedding(X_mlle,\n",
    "               \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# HLLE embedding of the digits dataset\n",
    "print(\"Computing Hessian LLE embedding\")\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='hessian')\n",
    "t0 = time()\n",
    "X_hlle = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plot_embedding(X_hlle,\n",
    "               \"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# LTSA embedding of the digits dataset\n",
    "print(\"Computing LTSA embedding\")\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
    "                                      method='ltsa')\n",
    "t0 = time()\n",
    "X_ltsa = clf.fit_transform(X)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "plot_embedding(X_ltsa,\n",
    "               \"Local Tangent Space Alignment of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# MDS  embedding of the digits dataset\n",
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
    "t0 = time()\n",
    "X_mds = clf.fit_transform(X)\n",
    "print(\"Done. Stress: %f\" % clf.stress_)\n",
    "plot_embedding(X_mds,\n",
    "               \"MDS embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Random Trees embedding of the digits dataset\n",
    "print(\"Computing Totally Random Trees embedding\")\n",
    "hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n",
    "                                       max_depth=5)\n",
    "t0 = time()\n",
    "X_transformed = hasher.fit_transform(X)\n",
    "pca = decomposition.TruncatedSVD(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_transformed)\n",
    "\n",
    "plot_embedding(X_reduced,\n",
    "               \"Random forest embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Spectral embedding of the digits dataset\n",
    "print(\"Computing Spectral embedding\")\n",
    "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n",
    "                                      eigen_solver=\"arpack\")\n",
    "t0 = time()\n",
    "X_se = embedder.fit_transform(X)\n",
    "\n",
    "plot_embedding(X_se,\n",
    "               \"Spectral embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plot_embedding(X_tsne,\n",
    "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
    "               (time() - t0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Посмотрим на датасет детектирования фрода:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравним точность классификации Цифр со снижением размерности PCA и Isomap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним прирост по точности на  Лог. Регрессии, почему нет?\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "log_reg = #\n",
    "\n",
    "skf = #\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    log_reg.fit(X_train, y_train)\n",
    "    print(log_reg.score(X_test, y_test))\n",
    "    \n",
    "    scores.append(log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(scores).mean(), np.array(scores).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним прирост по точности на линейных и нелинейных методах снижения размерности \n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "isomap = manifold.Isomap(n_components=30)\n",
    "# pca = PCA(n_components = 30)\n",
    "\n",
    "scores1 = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = isomap.fit_transform(X_train)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = isomap.transform(X_test)\n",
    "    \n",
    "    print(log_reg.score(X_test, y_test))\n",
    "    \n",
    "    scores1.append(log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(scores1).mean(), np.array(scores1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеринг текстов:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1. \n",
    "Примерить PCA к датасету `sklearn.datasets.fetch_olivetti_faces `. Визуализировать компоненты\n",
    "\n",
    "https://scikit-learn.org/0.19/datasets/olivetti_faces.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState(0)\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state=rng).data\n",
    "\n",
    "image_shape = (64, 64)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
