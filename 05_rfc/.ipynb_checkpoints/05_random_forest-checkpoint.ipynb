{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Введение в машинное обучение\n",
    "\n",
    "## Семинар #5\n",
    "\n",
    "### Екатерина Кондратьева\n",
    "\n",
    "ekaterina.kondrateva@skoltech.ru\n",
    "\n",
    "## Деревья решений (Decision Trees). Случайный лес (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Деревья решений (Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дерево принятия решений (также может называться деревом классификации или регрессионным деревом) — средство поддержки принятия решений, использующееся в машинном обучении, анализе данных и статистике. Структура дерева представляет собой «листья» и «ветки». На рёбрах («ветках») дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах — атрибуты, по которым различаются случаи. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение.  \n",
    "\n",
    "Источники:\n",
    "1. Лекция https://ru.coursera.org/lecture/supervised-learning/rieshaiushchiie-dieriev-ia-HZxD1 \n",
    "2. https://chrisalbon.com/machine_learning/trees_and_forests/visualize_a_decision_tree/\n",
    "3. https://habr.com/ru/post/171759/\n",
    "4. https://www.hse.ru/mirror/pubs/share/215285956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear algebra\n",
    "import numpy as np\n",
    "#data structures\n",
    "import pandas as pd\n",
    "#ml models\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVR\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#beautiful plots\n",
    "import seaborn as sns\n",
    "#linear regression\n",
    "import statsmodels.api as sm\n",
    "#set style for plots\n",
    "sns.set_style('darkgrid')\n",
    "#off the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier     #KNN\n",
    "from sklearn.linear_model import LogisticRegression    #Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap прошлого занятия: классификация на выборке ирисов kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=[0, 1]\n",
    "X = iris.data[:, [0, 1]]\n",
    "y = iris.target\n",
    "\n",
    "n_classes = 3\n",
    "plot_colors = ['g', 'gold', 'black']\n",
    "plot_step = 0.005\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=10).fit(X, y)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap='Accent')\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=[0, 1]\n",
    "X = iris.data[:, [0, 1]]\n",
    "y = iris.target\n",
    "\n",
    "n_classes = 3\n",
    "plot_colors = ['g', 'gold', 'black']\n",
    "plot_step = 0.005\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42).fit(X, y)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap='Accent')\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Деревья решений можно визуализировать:\n",
    "\n",
    "Пример классификации данного датасета\n",
    "    \n",
    "!['деревьеярешений'](https://scikit-learn.org/stable/_images/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как переобучиться на Деревьях Решений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее в коде закомментирована рисовалка решающего правила для деревьев решений. Если у вас она не ставится с первого раза - оставьте попытки, она не пригодится дальше нигде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = [0, 1]\n",
    "X = X_train[:, pair]\n",
    "y = y_train\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 30, random_state = 42).fit(X, y)  # min_samples_split --?\n",
    "\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "#                                  feature_names=['petal length', \n",
    "#                                                 'petal width'],  \n",
    "#                                  class_names=iris.target_names,  \n",
    "#                                  filled=True, rounded=True,\n",
    "#                                  special_characters=True)  \n",
    "# graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "print(clf.score(X_test[:, pair], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_leaf = 5, min_samples_split = 5, random_state = 42).fit(X, y)\n",
    "\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "#                                  feature_names=['petal length', \n",
    "#                                                 'petal width'],  \n",
    "#                                  class_names=iris.target_names,  \n",
    "#                                  filled=True, rounded=True,\n",
    "#                                  special_characters=True)  \n",
    "# graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "print(clf.score(X_test[:, pair], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на справку функции в `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор критерия:\n",
    "    http://www.machinelearning.ru/wiki/images/8/89/Sem3_trees.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "xx = np.linspace(0,1,50)\n",
    "plt.plot(xx, [2 * x * (1-x) for x in xx], label='gini')\n",
    "plt.plot(xx, [-x * np.log2(x) - (1-x) * np.log2(1 - x)  for x in xx], label='entropy')\n",
    "plt.plot(xx, [1 - max(x, 1-x) for x in xx], label='missclass')\n",
    "plt.xlabel('p+')\n",
    "plt.ylabel('criterion')\n",
    "plt.title('Критерии качества как функции от p (бинарная классификация)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия на Деревьях Решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap KNN \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() \n",
    "y[::2] += 1 * (0.5 - rng.rand(40))\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "clf = KNeighborsRegressor(n_neighbors = 5, \n",
    "                         ).fit(X, y)\n",
    "y_ = clf.predict(X_test)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, c='darkorange', label='data')\n",
    "plt.scatter(X_test, y_, c='red', label='data')\n",
    "plt.plot(X_test, y_, c='cornflowerblue', label='prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() \n",
    "y[::2] += 1 * (0.5 - rng.rand(40))\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "clf = SVR().fit(X, y)\n",
    "y_ = clf.predict(X_test)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, c='darkorange', label='data')\n",
    "plt.plot(X_test, y_, c='cornflowerblue', label='prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::2] += 1 * (0.5 - rng.rand(40))\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "clf = DecisionTreeRegressor().fit(X, y)\n",
    "y_ = clf.predict(X_test)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, c='darkorange', label='data')\n",
    "plt.plot(X_test, y_, c='cornflowerblue', label='prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeRegressor(min_samples_split = 10, min_samples_leaf = 5).fit(X, y)\n",
    "y_ = clf.predict(X_test)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, c='darkorange', label='data')\n",
    "plt.plot(X_test, y_, c='cornflowerblue', label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каком из этих трех случаев модель переобучилась? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему не воспроизводится результат DTC на дефолтных параметрах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Леса решений: Random Forest Classification (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target,test_size=0.3, stratify=wine.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42) \n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(log_reg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=42) \n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(dtc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(dtc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10000, random_state = 42, warm_start=True) \n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(rfc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(rfc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# посмотрим на выбранные характеристики модели\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на выбранные характеристики модели\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = wine.data.shape[1]\n",
    "plt.barh(range(n_feature), rfc.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_feature), wine.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "#plt.ylim(1)\n",
    "#plt.xlim(0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(X_train, columns=wine.feature_names)\n",
    "X_train.drop(['color_intensity','alcohol'], axis=1, inplace=True)\n",
    "\n",
    "X_test=pd.DataFrame(X_test, columns=wine.feature_names)\n",
    "X_test.drop(['color_intensity','alcohol'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=42) \n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(dtc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(dtc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = X_train.shape[1]\n",
    "plt.barh(range(rfc.feature_importances_.shape[0]), rfc.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_feature), X_train.columns)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "#plt.ylim(1)\n",
    "#plt.xlim(0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "rs = np.random.RandomState(33)\n",
    "d = pd.DataFrame(X_train)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.feature_names[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train = X_train[X_train.columns[:2]]\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test = X_test[X_test.columns[:2]]\n",
    "dtc = DecisionTreeClassifier(random_state=42) \n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(dtc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(dtc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = X_train.shape[1]\n",
    "plt.barh(range(n_feature), dtc.feature_importances_, align='center')\n",
    "#plt.yticks(np.arange(n_feature), X_train.columns)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 10, shuffle= True)\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "list_ = []\n",
    "kfold = KFold(n_splits = 10)\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('Fold #', i)\n",
    "    i+=1\n",
    "    dtc.fit(X_train,y_train)\n",
    "    list_.append(dtc.score(X_test,y_test))\n",
    "    print(dtc.score(X_test,y_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "list_1 = []\n",
    "kfold = KFold(n_splits = 10)\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "dtc = RandomForestClassifier()\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('Fold #', i)\n",
    "    i+=1\n",
    "    dtc.fit(X_train,y_train)\n",
    "    list_1.append(dtc.score(X_test,y_test))\n",
    "    print(dtc.score(X_test,y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list_), np.std(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list_1), np.std(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "scipy.stats.ttest_rel(list_1,list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос: как данную модель интерпретировать? Как Дерево решений построило решающее правило?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.  \n",
    "Сравнить решающее правило на Деревьях Решений и Случайных Лесов Решений для регрессионной задачи на выборке `sklearn.datasets.diabets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализации Линейной Регрессии по одному признаку\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.max(), y_test.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.fit_transform(pd.DataFrame(y_train))\n",
    "y_test = scaler.transform(pd.DataFrame(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non sclaed\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtc = DecisionTreeRegressor(random_state=42) \n",
    "dtc.fit( X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(dtc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(dtc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non scaled\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "dtc = RandomForestRegressor(random_state=42) \n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(dtc.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(dtc.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Вопросы для самопроверки:\n",
    "\n",
    "1. В чем отличие Decision Trees от Random Forest?\n",
    "2. На что влияют критерии построения решающего правила в деревьях?\n",
    "3. Как интерпретировать результат модели RFC?\n",
    "4. Почему  важно варьировать `max_depth` дерева?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "1. Дерево – интерпретируемый алгоритм (пока оно не очень глубокое). Random Forest – не интерпретируемый алгоритм\n",
    "1. Параметр глубины дерева max_depth нужно находить исходя из компромисс между underfitting и overfitting! Можно переобучиться и на тестовом датасете\n",
    "4. Random Forest борется с изъянами Decision Tree путем построения большого количества разных деревьев и их коллективного голосования.\n",
    "2. Построение дерева DTC и качество модели сильно зависит от того, удачно ли были выбраны сплиты в начале построения!  \n",
    "3. Стандартной реализации для задачи регрессии вам доступны критерии gini, entropy \n",
    "5. Качество Random Forest неубывает с увеличением деревьев (не происходит переобучения)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. \n",
    "Мы уже знаем 4 принципаильно разных классификатора: LR, KNN, SVC, RFC. \n",
    "Сравнить их точности предсказания на датасете `breast cancer`. Использовать классификаторы с натройками по умолчанию. Разбить данные на `train`  и  `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "random_state  = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc= SVC()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_state = 42\n",
    "models = <YOUR CODE>\n",
    "\n",
    "for model in models:\n",
    "    <YOUR CODE># fit the model\n",
    "    predictions = model.predict(X_test)\n",
    "    print(str(model)[:10], accuracy_score(y_test, predictions))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "pipe = Pipeline(<YOUR CODE>\n",
    ")\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [<YOUR CODE>\n",
    "]\n",
    "# Create grid search \n",
    "\n",
    "clf = GridSearchCV(<YOUR CODE>\n",
    "                  ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посомтреть результаты поиска по гиперапараметрам:\n",
    "\n",
    "Вопрос: какая модель называется лучшей (по каким критериям)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3 (Домашнее задание).\n",
    "\n",
    "1. Построить `feature importances` для каждого классифкатора, посмотреть, какие признаки учавствуют в построении решающего правила\n",
    "2. Вывести индексы пациентов тестовой выборки, на которых ошибаются классификаторы\n",
    "3. Вывести все параметры пациентов, сравнить со средними значениями по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 4 (Домашнее задание). \n",
    "Варьировать параметры классификатора для достижения лучшего `score` на тестовой выборке. Внести результат в турнирную таблицу.\n",
    "\n",
    "Что можно сказать по поводу переобучения при таком дизайне эксперимента?    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
